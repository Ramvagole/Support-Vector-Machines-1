{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94d0a0cf-1feb-4086-a60f-643057108033",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q1):-\n",
    "The mathematical formula for a linear Support Vector Machine (SVM) can be expressed as follows:\n",
    "\n",
    "Given a training dataset with input features represented as vectors (x1,x2,x3,....,xn) and\n",
    "their corresponding binary class labels (y1,y2,..,yn)  where yi belongs to either + 1 or -1\n",
    "the goal of the linear SVM is to find the optimal hyperplane that best separates the two classes.\n",
    "\n",
    "The hyperplane can be represented as:\n",
    "\n",
    " w.x+ b = 0\n",
    "\n",
    "Where:\n",
    " w is the weight vector (normal vector) perpendicular to the hyperplane.\n",
    " x represents the input feature vector.\n",
    " b is the bias term, which determines the offset of the hyperplane from the origin.\n",
    "\n",
    "The decision function of the linear SVM is given by:\n",
    "\n",
    " x= w.x+ b \n",
    "\n",
    "The predicted class for a given input \\(\\mathbf{x}\\) is determined by the sign of f(x)\n",
    "f(x)>0, the sample is classified as the positive class.\n",
    "f(x)<0, the sample is classified as the negative class.\n",
    "\n",
    "The training objective of the linear SVM is to find \\(\\mathbf{w}\\) and \\(b\\) that maximize the margin between the two classes while minimizing \n",
    "the classification error. The margin is the distance between the hyperplane and the closest data points of the two classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a01013dc-1231-4225-818b-7998cc4054f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q2):-\n",
    "\n",
    "The objective function of a linear Support Vector Machine (SVM) is to find the parameters w and b that define the optimal hyperplane, \n",
    "which maximizes the margin between the two classes while minimizing the classification error.\n",
    "\n",
    "In the case of a linearly separable dataset, the objective function of the linear SVM is to maximize the margin while ensuring that all data points \n",
    "are correctly classified. The margin is defined as the distance between the hyperplane and the closest data points (support vectors) of the two\n",
    "classes.\n",
    "\n",
    "The objective function is typically formulated as follows:\n",
    "\n",
    "Minimize: 1/2 ||w||^2\n",
    "\n",
    "Subject to the constraints:\n",
    "   yi (w.xi+b)>1, for i=1,2,...,n\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7c6791a-b01c-49c9-a806-28a465e0e99a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q3):-\n",
    "The kernel trick is a fundamental concept in Support Vector Machines (SVM) that allows SVMs to efficiently handle non-linearly separable data by\n",
    "implicitly transforming the original feature space into a higher-dimensional space. It is a clever mathematical technique that avoids the explicit \n",
    "computation of the transformed feature space, making SVMs computationally more efficient.\n",
    "\n",
    "In the standard linear SVM, the decision boundary is a hyperplane in the original feature space. However, many real-world datasets are not linearly\n",
    "separable, and finding a linear boundary would result in poor classification performance. The kernel trick addresses this limitation by introducing a \n",
    "kernel function, which implicitly computes the dot product between the transformed feature vectors in a higher-dimensional space, without explicitly\n",
    "transforming the data into that space.\n",
    "\n",
    "The general idea is to find a mapping function ϕ(x)  that maps the input feature vector x into a higher-dimensional space, often referred to as the \n",
    "feature space. The kernel function K(xi,xj) then calculates the dot product of the transformed feature vectors ϕ(xi)  and ϕ(xj) without having to \n",
    "explicitly compute ϕ(xi) and ϕ(xj).\n",
    "\n",
    "The kernel function K(xi,xj) takes two input feature vectors xi and xj and computes their dot product in the feature space:\n",
    "    K(xi,xj)=ϕ(x)⋅ϕ(xj)\n",
    "    \n",
    "The key benefit of using the kernel trick is that it avoids the computational overhead of explicitly transforming the data into the \n",
    "higher-dimensional feature space, which can become impractical or even infeasible for very high-dimensional spaces. Instead, the kernel\n",
    "function allows SVM to operate directly in the original feature space while effectively leveraging the benefits of working in a higher-dimensional\n",
    "space.\n",
    "\n",
    "Some commonly used kernel functions include:\n",
    "\n",
    "Linear Kernel: K(xi,xj)=xi⋅xj\n",
    "Polynomial Kernel: K(xi,x j)=(xi⋅xj+c)d\n",
    "\n",
    "The choice of the kernel function depends on the nature of the data and the problem at hand. By selecting an appropriate kernel function, \n",
    "SVMs can effectively handle complex decision boundaries and capture non-linear relationships between features, making them powerful and versatile \n",
    "classifiers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27d455a5-79b4-4b43-b3f6-1fb8ff5068a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q4):-\n",
    "In Support Vector Machines (SVM), support vectors play a crucial role in defining the optimal hyperplane that separates the data into different classes. Support vectors are the data points from the training set that lie closest to the decision boundary (hyperplane). These points are the most informative for defining the decision boundary because they are the ones that \"support\" the placement of the hyperplane.\n",
    "\n",
    "To illustrate the role of support vectors, let's consider a simple example with a 2-dimensional dataset and a linear SVM.\n",
    "\n",
    "Example:\n",
    "Suppose we have a binary classification problem with two classes: circles (O) and crosses (X). The dataset is as follows:\n",
    "\n",
    "Circles (O): (2, 2), (3, 3), (4, 4)\n",
    "Crosses (X): (1, 4), (2, 5), (3, 6)\n",
    "The goal of the SVM is to find the best hyperplane (line in 2D) that separates the circles from the crosses.\n",
    "Let's assume the SVM finds the following hyperplane as the decision boundary: \n",
    "x2=x1+1.\n",
    "\n",
    "In this case, the support vectors are the points closest to the decision boundary. Let's find them and mark them in the plot:\n",
    "\n",
    "Circles (O): (2, 2), (3, 3)\n",
    "Crosses (X): (3, 6)\n",
    "The support vectors are the data points that lie closest to the decision boundary. In this example, they are (2, 2), (3, 3), and (3, 6).\n",
    "These points determine the position of the hyperplane, and the distance between the hyperplane and the support vectors is known as the margin.\n",
    "\n",
    "In SVM, the objective is to maximize the margin while minimizing the classification error. The hyperplane is chosen in such a way that it maximizes\n",
    "the distance between the support vectors, as the larger the margin, the more confident the model is in its classification.\n",
    "\n",
    "Support vectors are crucial for the SVM's robustness and generalization because they represent the most critical data points that define the decision\n",
    "boundary. SVM focuses on these support vectors rather than the entire dataset, which allows it to be efficient and effective, especially in\n",
    "high-dimensional spaces. Additionally, SVM's decision boundary depends only on the support vectors, which makes it less sensitive to outliers that\n",
    "may exist in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ee2091c-46d5-4511-bd65-ae443588b7da",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q5):-\n",
    "To illustrate the concepts of hyperplane, marginal plane, soft margin, and hard margin in SVM, let's consider a simple 2-dimensional dataset \n",
    "with two classes: circles (O) and crosses (X).\n",
    "Suppose we have the following data points:\n",
    "Circles (O): (2, 3), (3, 5), (4, 4), (5, 5)\n",
    "Crosses (X): (1, 1), (2, 2), (4, 2), (5, 3)\n",
    "\n",
    "Hyperplane:\n",
    "The hyperplane is the decision boundary that separates the two classes. In a 2-dimensional space, the hyperplane is a line. \n",
    "SVM aims to find the optimal hyperplane that maximizes the margin between the two classes.\n",
    "\n",
    "Example hyperplane: x2=x1−1\n",
    "\n",
    "Marginal Plane:\n",
    "The marginal plane, also known as the decision plane, is the hyperplane that lies parallel to the optimal hyperplane and passes through the support\n",
    "vectors. The marginal plane is important because it defines the margin, which is the distance between the support vectors and the hyperplane.\n",
    "Example marginal plane corresponding to the hyperplane: x2=x1−1\n",
    "\n",
    "Hard Margin SVM:\n",
    "In hard margin SVM, the objective is to find a hyperplane that perfectly separates the two classes with no misclassifications. \n",
    "This means that all data points of one class are on one side of the hyperplane, and all data points of the other class are on the other side. \n",
    "Hard margin SVM works only when the data is linearly separable.\n",
    "Example hyperplane for hard margin SVM: x2=x1−1\n",
    "\n",
    "Soft Margin SVM:\n",
    "In soft margin SVM, the objective is to find a hyperplane that separates the two classes with some tolerance for misclassifications (errors) \n",
    "and allows for a small number of data points to be on the wrong side of the margin or even misclassified. Soft margin SVM is used when the data\n",
    "is not perfectly separable by a hyperplane.\n",
    "Example hyperplane for soft margin SVM: x2=x1−1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b217c66-8212-446e-80c0-bbdec3316b7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q6):-\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load the Iris dataset\n",
    "iris = datasets.load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "# Split the dataset into 80% training and 20% testing\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "# Create the SVM classifier with a linear kernel\n",
    "# Note: C is the regularization parameter. Smaller C values correspond to stronger regularization.\n",
    "C = 1.0\n",
    "svm_classifier = SVC(kernel='linear', C=C)\n",
    "\n",
    "# Train the classifier on the training set\n",
    "svm_classifier.fit(X_train, y_train)\n",
    "# Predict labels for the testing set\n",
    "y_pred = svm_classifier.predict(X_test)\n",
    "\n",
    "# Compute the accuracy of the model on the testing set\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy:.2f}\")\n",
    "# Function to plot the decision boundaries\n",
    "def plot_decision_boundaries():\n",
    "    # We'll use the first two features for visualization\n",
    "    feature1 = 0\n",
    "    feature2 = 1\n",
    "\n",
    "    # Create a meshgrid to plot the decision boundaries\n",
    "    h = 0.02  # Step size in the mesh\n",
    "    x_min, x_max = X[:, feature1].min() - 1, X[:, feature1].max() + 1\n",
    "    y_min, y_max = X[:, feature2].min() - 1, X[:, feature2].max() + 1\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n",
    "\n",
    "    # Make predictions on the meshgrid points\n",
    "    Z = svm_classifier.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "    Z = Z.reshape(xx.shape)\n",
    "\n",
    "    # Plot the decision boundaries and data points\n",
    "    plt.contourf(xx, yy, Z, cmap=plt.cm.coolwarm, alpha=0.8)\n",
    "    plt.scatter(X[:, feature1], X[:, feature2], c=y, cmap=plt.cm.coolwarm, edgecolors='k')\n",
    "    plt.xlabel(iris.feature_names[feature1])\n",
    "    plt.ylabel(iris.feature_names[feature2])\n",
    "    plt.title(\"Decision Boundaries of Linear SVM\")\n",
    "    plt.show()\n",
    "\n",
    "# Plot the decision boundaries\n",
    "plot_decision_boundaries()\n",
    "# Try different values of C\n",
    "C_values = [0.01, 0.1, 1.0, 10.0, 100.0]\n",
    "\n",
    "# Train and evaluate the model for each value of C\n",
    "for C in C_values:\n",
    "    svm_classifier = SVC(kernel='linear', C=C)\n",
    "    svm_classifier.fit(X_train, y_train)\n",
    "    y_pred = svm_classifier.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    print(f\"C = {C}, Accuracy: {accuracy:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eaa7451-1ac9-4f8c-9d3c-87f0c9d330d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q):-\n",
    "BONUS TASK:\n",
    "import numpy as np\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Define the Linear SVM Classifier\n",
    "class LinearSVM:\n",
    "    def __init__(self, learning_rate=0.01, max_iters=1000, C=1.0):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.max_iters = max_iters\n",
    "        self.C = C\n",
    "        self.W = None\n",
    "        self.b = None\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        n_samples, n_features = X.shape\n",
    "\n",
    "        # Initialize weight vector and bias term\n",
    "        self.W = np.zeros(n_features)\n",
    "        self.b = 0\n",
    "\n",
    "        # Training using SMO\n",
    "        for _ in range(self.max_iters):\n",
    "            for i in range(n_samples):\n",
    "                if y[i] * (np.dot(self.W, X[i]) + self.b) < 1:\n",
    "                    self.W = self.W + self.learning_rate * (self.C * y[i] * X[i] - 2 * self.C * self.W)\n",
    "                    self.b = self.b + self.learning_rate * self.C * y[i]\n",
    "\n",
    "    def predict(self, X):\n",
    "        return np.sign(np.dot(X, self.W) + self.b)\n",
    "\n",
    "# Load the Iris dataset\n",
    "iris = datasets.load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "# Split the dataset into 80% training and 20% testing\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train the linear SVM classifier from scratch\n",
    "svm_classifier_scratch = LinearSVM(learning_rate=0.01, max_iters=1000, C=1.0)\n",
    "svm_classifier_scratch.fit(X_train, y_train)\n",
    "\n",
    "# Predict labels for the testing set\n",
    "y_pred_scratch = svm_classifier_scratch.predict(X_test)\n",
    "\n",
    "# Compute the accuracy of the model on the testing set\n",
    "accuracy_scratch = accuracy_score(y_test, y_pred_scratch)\n",
    "print(f\"Accuracy of SVM from scratch: {accuracy_scratch:.2f}\")\n",
    "\n",
    "\n",
    "Now, let's use the scikit-learn implementation of the linear SVM classifier and compare the accuracy:\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# Train the linear SVM classifier using scikit-learn\n",
    "svm_classifier_sklearn = SVC(kernel='linear', C=1.0)\n",
    "svm_classifier_sklearn.fit(X_train, y_train)\n",
    "\n",
    "# Predict labels for the testing set using scikit-learn implementation\n",
    "y_pred_sklearn = svm_classifier_sklearn.predict(X_test)\n",
    "\n",
    "# Compute the accuracy of the model on the testing set using scikit-learn implementation\n",
    "accuracy_sklearn = accuracy_score(y_test, y_pred_sklearn)\n",
    "print(f\"Accuracy of SVM using scikit-learn: {accuracy_sklearn:.2f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
